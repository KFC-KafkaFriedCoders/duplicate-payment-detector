package com.kafka.duplicatedetector;

import com.kafka.duplicatedetector.functions.UserKeySelector;
import com.kafka.duplicatedetector.functions.ReceiptToFilteredMapper;
import com.kafka.duplicatedetector.functions.DuplicateDetectionWindowFunction;
import com.kafka.duplicatedetector.model.ReceiptData;
import com.kafka.duplicatedetector.model.FilteredReceiptData;
import com.kafka.duplicatedetector.model.DuplicateAlert;
import com.kafka.duplicatedetector.utils.AppProperties;
import com.kafka.duplicatedetector.utils.DuplicateAlertJsonSerializationSchema;
import com.kafka.duplicatedetector.utils.SimpleAvroDeserializationSchema;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Properties;

/**
 * =======================================================
 * ì¤‘ë³µ ê²°ì œ íƒì§€ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
 * =======================================================
 * 
 * ğŸ“‹ ê¸°ëŠ¥ ê°œìš”:
 * - Kafkaì—ì„œ Avro í˜•ì‹ì˜ ì˜ìˆ˜ì¦ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì½ì–´ì˜´
 * - ë™ì¼ ì‚¬ìš©ìê°€ ì§§ì€ ì‹œê°„ ë‚´ì— ì—¬ëŸ¬ ë§¤ì¥ì—ì„œ ê²°ì œí•˜ëŠ” íŒ¨í„´ íƒì§€
 * - ì‚¬ìš©ìë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ìœˆë„ìš° ê¸°ë°˜ ì¤‘ë³µ ê²°ì œ ë¶„ì„
 * - ì¤‘ë³µ íƒì§€ ì‹œ ì•Œë¦¼ì„ JSON í˜•ì‹ìœ¼ë¡œ Kafkaì— ì „ì†¡
 * 
 * ğŸ”„ ë°ì´í„° í”Œë¡œìš°:
 * Kafka(Avro) â†’ Map(í•„í„°ë§) â†’ KeyBy(ì‚¬ìš©ì) â†’ Window(ì‹œê°„) â†’ Process(íƒì§€) â†’ Kafka(JSON)
 * 
 * âš™ï¸ ì£¼ìš” ì„¤ì •:
 * - ì…ë ¥: test-topic (Avro í˜•ì‹)
 * - ì¶œë ¥: payment_same_user (JSON í˜•ì‹)
 * - ìœˆë„ìš°: 10ì´ˆ (ì„¤ì • ê°€ëŠ¥)
 * - íƒì§€ ì¡°ê±´: ë™ì¼ ì‚¬ìš©ì, ì„œë¡œ ë‹¤ë¥¸ ë§¤ì¥, ìœˆë„ìš° ì‹œê°„ ë‚´
 */
public class DuplicatePaymentDetectorApp {
    private static final Logger LOG = LoggerFactory.getLogger(DuplicatePaymentDetectorApp.class);
    
    /**
     * =======================================================
     * ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì 
     * =======================================================
     */
    public static void main(String[] args) throws Exception {
        // ğŸ“Š ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ë¡œê·¸
        logApplicationInfo();
        
        // ğŸ¯ Flink ì‹¤í–‰ í™˜ê²½ êµ¬ì„±
        StreamExecutionEnvironment env = createFlinkEnvironment();
        
        // ğŸ“¥ Kafka Consumer ì„¤ì • (ì…ë ¥ ìŠ¤íŠ¸ë¦¼)
        FlinkKafkaConsumer<ReceiptData> consumer = createKafkaConsumer();
        
        // ğŸ“¤ Kafka Producer ì„¤ì • (ì¶œë ¥ ìŠ¤íŠ¸ë¦¼)
        FlinkKafkaProducer<DuplicateAlert> producer = createKafkaProducer();
        
        // ğŸ”„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
        buildDataPipeline(env, consumer, producer);
        
        // ğŸš€ ì¡ ì‹¤í–‰
        env.execute("Duplicate Payment Detection Application");
    }
    
    /**
     * =======================================================
     * ì• í”Œë¦¬ì¼€ì´ì…˜ ì •ë³´ ë¡œê¹…
     * =======================================================
     */
    private static void logApplicationInfo() {
        LOG.info("=".repeat(60));
        LOG.info("ğŸš€ Starting Duplicate Payment Detection Application");
        LOG.info("=".repeat(60));
        LOG.info("ğŸ“‹ Application: {} v{}", AppProperties.getAppName(), AppProperties.getAppVersion());
        LOG.info("ğŸ“¥ Source Topic: {} â†’ ğŸ“¤ Sink Topic: {}", AppProperties.getSourceTopic(), AppProperties.getSinkTopic());
        LOG.info("ğŸ”§ Bootstrap Servers: {}", AppProperties.getBootstrapServers());
        LOG.info("ğŸ‘¥ Consumer Group: {}", AppProperties.getConsumerGroup());
        LOG.info("â±ï¸ Window Size: {}s", AppProperties.getWindowSizeSeconds());
        LOG.info("ğŸ” Checkpoint Interval: {}ms", AppProperties.getCheckpointInterval());
        LOG.info("=".repeat(60));
    }
    
    /**
     * =======================================================
     * Flink ì‹¤í–‰ í™˜ê²½ ìƒì„± ë° êµ¬ì„±
     * =======================================================
     * 
     * ğŸ”§ ì„¤ì • í•­ëª©:
     * - ì²´í¬í¬ì¸íŠ¸ ê°„ê²© (ì¥ì•  ë³µêµ¬ìš©)
     * - ë³‘ë ¬ ì²˜ë¦¬ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš© (í´ëŸ¬ìŠ¤í„° í™˜ê²½ì— ë”°ë¼ ìë™ ì¡°ì •)
     */
    private static StreamExecutionEnvironment createFlinkEnvironment() {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // âœ… ì²´í¬í¬ì¸íŠ¸ í™œì„±í™” (ì¥ì•  ë³µêµ¬ìš©)
        env.enableCheckpointing(AppProperties.getCheckpointInterval());
        
        LOG.info("âœ… Flink environment configured with checkpoint interval: {}ms", 
                AppProperties.getCheckpointInterval());
        return env;
    }
    
    /**
     * =======================================================
     * Kafka Consumer ìƒì„± (ì…ë ¥ ìŠ¤íŠ¸ë¦¼)
     * =======================================================
     * 
     * ğŸ“¥ ì—­í• :
     * - test-topicì—ì„œ Avro í˜•ì‹ì˜ ì˜ìˆ˜ì¦ ë°ì´í„° ì½ê¸°
     * - ìë™ìœ¼ë¡œ ReceiptData ê°ì²´ë¡œ ì—­ì§ë ¬í™”
     * - Consumer Groupìœ¼ë¡œ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
     */
    private static FlinkKafkaConsumer<ReceiptData> createKafkaConsumer() {
        Properties consumerProps = createConsumerProperties();
        
        FlinkKafkaConsumer<ReceiptData> consumer = new FlinkKafkaConsumer<>(
            AppProperties.getSourceTopic(),
            new SimpleAvroDeserializationSchema(),
            consumerProps
        );
        
        LOG.info("ğŸ“¥ Kafka Consumer created for topic: {}", AppProperties.getSourceTopic());
        return consumer;
    }
    
    /**
     * =======================================================
     * Kafka Producer ìƒì„± (ì¶œë ¥ ìŠ¤íŠ¸ë¦¼)
     * =======================================================
     * 
     * ğŸ“¤ ì—­í• :
     * - DuplicateAlert ê°ì²´ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”
     * - payment_same_user í† í”½ìœ¼ë¡œ ì „ì†¡
     * - At-Least-Once ë³´ì¥ (ë°ì´í„° ì†ì‹¤ ë°©ì§€)
     */
    private static FlinkKafkaProducer<DuplicateAlert> createKafkaProducer() {
        Properties producerProps = createProducerProperties();
        
        FlinkKafkaProducer<DuplicateAlert> producer = new FlinkKafkaProducer<>(
            AppProperties.getSinkTopic(),
            new DuplicateAlertJsonSerializationSchema(AppProperties.getSinkTopic()),
            producerProps,
            FlinkKafkaProducer.Semantic.AT_LEAST_ONCE
        );
        
        LOG.info("ğŸ“¤ Kafka Producer created for topic: {}", AppProperties.getSinkTopic());
        return producer;
    }
    
    /**
     * =======================================================
     * ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
     * =======================================================
     * 
     * ğŸ”„ ì²˜ë¦¬ ìˆœì„œ:
     * 1. Kafkaì—ì„œ ì˜ìˆ˜ì¦ ë°ì´í„° ì½ê¸°
     * 2. í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ê²½ëŸ‰í™”
     * 3. ì‚¬ìš©ìë³„ë¡œ ê·¸ë£¹í™”
     * 4. ì‹œê°„ ìœˆë„ìš° ì ìš©
     * 5. ì¤‘ë³µ ê²°ì œ íƒì§€ ì²˜ë¦¬
     * 6. ì•Œë¦¼ì„ Kafkaë¡œ ì „ì†¡
     */
    private static void buildDataPipeline(StreamExecutionEnvironment env, 
                                         FlinkKafkaConsumer<ReceiptData> consumer,
                                         FlinkKafkaProducer<DuplicateAlert> producer) {
        
        // ğŸ“¥ 1. ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì½ê¸°
        DataStream<ReceiptData> receiptStream = env.addSource(consumer)
            .name("Receipt Data Source");
        
        // ğŸ” 2. ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ë° ë¡œê¹…
        DataStream<ReceiptData> validatedStream = receiptStream
            .filter(receipt -> {
                if (receipt == null) {
                    LOG.warn("âš ï¸ Received null receipt");
                    return false;
                }
                if (LOG.isDebugEnabled()) {
                    LOG.debug("ğŸ“¨ Received receipt from user: {} at store: {}", 
                             receipt.getUserId(), receipt.getStoreName());
                }
                return true;
            })
            .name("Receipt Validation");
        
        // ğŸ¯ 3. í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œ (ë°ì´í„° ê²½ëŸ‰í™”)
        DataStream<FilteredReceiptData> filteredStream = validatedStream
            .map(new ReceiptToFilteredMapper())
            .name("Receipt Filtering");
        
        // ğŸ”‘ 4. ì‚¬ìš©ìë³„ ê·¸ë£¹í™” ë° ìœˆë„ìš° ì ìš©
        DataStream<DuplicateAlert> alertStream = filteredStream
            .keyBy(new UserKeySelector())
            .window(TumblingProcessingTimeWindows.of(Time.seconds(AppProperties.getWindowSizeSeconds())))
            .process(new DuplicateDetectionWindowFunction())
            .name("Duplicate Detection");
        
        // ğŸ“¤ 5. ê²°ê³¼ë¥¼ Kafkaë¡œ ì „ì†¡
        alertStream.addSink(producer)
            .name("Duplicate Alert Sink");
        
        // ğŸ–¥ï¸ 6. ì½˜ì†” ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        alertStream.print("ğŸš¨ DUPLICATE ALERT");
        
        LOG.info("âœ… Data pipeline configured successfully");
        LOG.info("ğŸ”„ Pipeline: Source â†’ Filter â†’ KeyBy(User) â†’ Window({}s) â†’ Process â†’ Sink", 
                AppProperties.getWindowSizeSeconds());
    }
    
    /**
     * =======================================================
     * Kafka Consumer ì„¤ì • ìƒì„±
     * =======================================================
     * 
     * ğŸ”§ ì£¼ìš” ì„¤ì •:
     * - bootstrap.servers: Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ
     * - group.id: Consumer Group (ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€)
     * - auto.offset.reset: ì²˜ìŒ ì‹¤í–‰ ì‹œ latestë¶€í„° ì½ê¸°
     */
    private static Properties createConsumerProperties() {
        Properties props = new Properties();
        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, AppProperties.getBootstrapServers());
        props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, AppProperties.getConsumerGroup());
        props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");
        
        LOG.debug("ğŸ“‹ Consumer properties configured");
        return props;
    }
    
    /**
     * =======================================================
     * Kafka Producer ì„¤ì • ìƒì„±
     * =======================================================
     * 
     * ğŸ”§ ì£¼ìš” ì„¤ì •:
     * - acks=all: ëª¨ë“  ë³µì œë³¸ì— ì“°ê¸° ì™„ë£Œ í›„ ì‘ë‹µ (ì•ˆì •ì„±)
     * - retries=3: ì‹¤íŒ¨ ì‹œ 3ë²ˆ ì¬ì‹œë„
     * - ë°°ì¹˜ ì²˜ë¦¬ ë° ì••ì¶• ìµœì í™” ì„¤ì •
     */
    private static Properties createProducerProperties() {
        Properties props = new Properties();
        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, AppProperties.getBootstrapServers());
        props.setProperty(ProducerConfig.ACKS_CONFIG, "all");
        props.setProperty(ProducerConfig.RETRIES_CONFIG, "3");
        props.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy");
        props.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, "16384");
        props.setProperty(ProducerConfig.LINGER_MS_CONFIG, "5");
        
        LOG.debug("ğŸ“‹ Producer properties configured");
        return props;
    }
}
